{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740296b9-a446-4c41-a3b1-9541b002c8f9",
   "metadata": {},
   "source": [
    "## Torch+Ignite CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec030b8c-7521-460f-ab9f-3fdf3d181860",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch  2.4.0\n",
      "torchvision  0.19.0\n",
      "ignite  0.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"pytorch \",torch.version.__version__)\n",
    "import torchvision\n",
    "print(\"torchvision \",torchvision.version.__version__)\n",
    "import ignite\n",
    "print(\"ignite \",ignite.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42638466-037b-4ec4-ae15-47e3e52f15d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Libraries used:\n",
    "1. PyTorch - [Main](https://pytorch.org/) / [conda channel](https://anaconda.org/pytorch/repo/files)\n",
    "2. Pytorch Ignite - https://pytorch-ignite.ai/\n",
    "3. Torchvision - https://pytorch.org/vision/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5795a-b6d6-4186-aa3a-5ba7afae39df",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65725c54-af03-4720-9b60-179ad496f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the training and test sets\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Custom transformation to scale normalized values back to unit [0, 1]\n",
    "class ToUnitRange:\n",
    "    def __call__(self, tensor):\n",
    "        return (tensor + 1) / 2\n",
    "\n",
    "# Define transformations for the training and test sets\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.0,), (1.0,))  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "# Example usage with CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader= torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ff1dff-2c9b-47a7-bfd9-0e9c4479a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model and optimizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15cee7e2-ab71-449a-a689-312224b9ebba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Ignite training loop\n",
    "\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine import Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import global_step_from_engine\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "# Create the evaluator\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(),\n",
    "    'loss': Loss(criterion)\n",
    "}\n",
    "evaluator = create_supervised_evaluator(model, metrics=metrics)\n",
    "\n",
    "# Create a checkpoint handler\n",
    "checkpoint_handler = ModelCheckpoint(\n",
    "    dirname='./checkpoints',\n",
    "    filename_prefix='cifar10',\n",
    "    n_saved=3,\n",
    "    create_dir=True,\n",
    "    require_empty=False,\n",
    "    atomic=True,\n",
    "    include_self=True,\n",
    "    global_step_transform=global_step_from_engine(trainer)\n",
    "    )\n",
    "to_save = {'model': model, 'optimizer': optimizer, 'trainer': trainer}\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def run_validation():\n",
    "    evaluator.run(testloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Epoch: {trainer.state.epoch},  Validation accuracy: {metrics['accuracy']}\",\n",
    "          f\"Loss: {metrics['loss']:.3f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=3))\n",
    "def run_training_loss():\n",
    "    evaluator.run(trainloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Epoch: {trainer.state.epoch},  Training accuracy: {metrics['accuracy']}\",\n",
    "          f\"Loss: {metrics['loss']:.3f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_checkpoint(engine):\n",
    "   checkpoint_handler(engine, to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0582b222-cd4f-464b-b5d0-828b8e17c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsn\\AppData\\Local\\Temp\\ipykernel_12968\\922409818.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./checkpoints/cifar10_checkpoint_2.pt')\n"
     ]
    }
   ],
   "source": [
    "# load saved checkpoint\n",
    "checkpoint = torch.load('./checkpoints/cifar10_checkpoint_2.pt')\n",
    "checkpoint_handler.load_objects(to_save, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d86429cf-43cc-4306-b940-882a3f7f05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,  Validation accuracy: 0.6136 Loss: 1.570\n",
      "Epoch: 2,  Validation accuracy: 0.6114 Loss: 1.598\n",
      "Epoch: 3,  Validation accuracy: 0.6123 Loss: 1.617\n",
      "Epoch: 4,  Validation accuracy: 0.6083 Loss: 1.608\n",
      "Epoch: 4,  Training accuracy: 0.85746 Loss: 0.398\n",
      "Epoch: 5,  Validation accuracy: 0.6102 Loss: 1.621\n",
      "Epoch: 6,  Validation accuracy: 0.6092 Loss: 1.615\n",
      "Epoch: 7,  Validation accuracy: 0.598 Loss: 1.669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 5474\n",
       "\tepoch: 7\n",
       "\tepoch_length: 782\n",
       "\tmax_epochs: 7\n",
       "\toutput: 0.5639845728874207\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run more epochs\n",
    "EPOCHS = 7\n",
    "\n",
    "# Determine the max number of epochs to run\n",
    "if trainer.state is not None:\n",
    "    current_epoch = trainer.state.epoch\n",
    "    max_epochs = current_epoch + EPOCHS\n",
    "else:\n",
    "    max_epochs = EPOCHS\n",
    "\n",
    "trainer.run(trainloader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6161e74-a3bb-45bd-ab7e-e1cc676eec88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
