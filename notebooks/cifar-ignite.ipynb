{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740296b9-a446-4c41-a3b1-9541b002c8f9",
   "metadata": {},
   "source": [
    "## Torch+Ignite CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec030b8c-7521-460f-ab9f-3fdf3d181860",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch  2.4.0\n",
      "torchvision  0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsn\\scoop\\apps\\mambaforge\\current\\envs\\pytorch\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignite  0.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"pytorch \",torch.version.__version__)\n",
    "import torchvision\n",
    "print(\"torchvision \",torchvision.version.__version__)\n",
    "import ignite\n",
    "print(\"ignite \",ignite.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42638466-037b-4ec4-ae15-47e3e52f15d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Libraries used:\n",
    "1. PyTorch - [Main](https://pytorch.org/) / [conda channel](https://anaconda.org/pytorch/repo/files)\n",
    "2. Pytorch Ignite - https://pytorch-ignite.ai/\n",
    "3. Torchvision - https://pytorch.org/vision/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5795a-b6d6-4186-aa3a-5ba7afae39df",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65725c54-af03-4720-9b60-179ad496f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the training and test sets\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Custom transformation to scale normalized values back to unit [0, 1]\n",
    "class ToUnitRange:\n",
    "    def __call__(self, tensor):\n",
    "        return (tensor + 1) / 2\n",
    "\n",
    "# Define transformations for the training and test sets\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.0,), (1.0,))  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "# Example usage with CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader= torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ff1dff-2c9b-47a7-bfd9-0e9c4479a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model and optimizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cee7e2-ab71-449a-a689-312224b9ebba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Ignite training loop\n",
    "\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine import Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import global_step_from_engine\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "# Create the evaluator\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(),\n",
    "    'loss': Loss(criterion)\n",
    "}\n",
    "evaluator = create_supervised_evaluator(model, metrics=metrics)\n",
    "\n",
    "# Create a checkpoint handler\n",
    "checkpoint_handler = ModelCheckpoint(\n",
    "    dirname='./checkpoints',\n",
    "    filename_prefix='cifar10',\n",
    "    n_saved=3,\n",
    "    create_dir=True,\n",
    "    require_empty=False,\n",
    "    atomic=True,\n",
    "    include_self=True,\n",
    "    global_step_transform=global_step_from_engine(trainer)\n",
    "    )\n",
    "to_save = {'model': model, 'optimizer': optimizer, 'trainer': trainer}\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def run_validation():\n",
    "    evaluator.run(testloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Epoch: {trainer.state.epoch},  Validation accuracy: {metrics['accuracy']}\",\n",
    "          f\"Loss: {metrics['loss']:.3f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=4))\n",
    "@trainer.on(Events.COMPLETED)\n",
    "def run_training_loss():\n",
    "    evaluator.run(trainloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Epoch: {trainer.state.epoch},  Training accuracy: {metrics['accuracy']}\",\n",
    "          f\"Loss: {metrics['loss']:.3f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_checkpoint(engine):\n",
    "   checkpoint_handler(engine, to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0582b222-cd4f-464b-b5d0-828b8e17c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsn\\AppData\\Local\\Temp\\ipykernel_3672\\2380063985.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: cifar10_checkpoint_14.pt\n"
     ]
    }
   ],
   "source": [
    "# load saved checkpoint\n",
    "import os\n",
    "from ignite.handlers import Checkpoint\n",
    "#checkpoint = torch.load('./checkpoints/cifar10_checkpoint_3.pt')\n",
    "#Checkpoint.load_objects(to_save, checkpoint)\n",
    "\n",
    "# Directory containing checkpoints\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Get list of all checkpoint files\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('cifar10_checkpoint') and f.endswith('.pt')]\n",
    "\n",
    "# Sort files by modification time in descending order\n",
    "checkpoint_files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "\n",
    "# Load the latest checkpoint\n",
    "latest_checkpoint = checkpoint_files[0]\n",
    "checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load objects from the checkpoint\n",
    "Checkpoint.load_objects(to_save, checkpoint)\n",
    "\n",
    "print(f\"Loaded checkpoint: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86429cf-43cc-4306-b940-882a3f7f05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11,  Validation accuracy: 0.605 Loss: 1.742\n",
      "Epoch: 12,  Validation accuracy: 0.5998 Loss: 1.789\n",
      "Epoch: 12,  Training accuracy: 0.85292 Loss: 0.408\n",
      "Epoch: 13,  Validation accuracy: 0.6007 Loss: 1.842\n",
      "Epoch: 14,  Validation accuracy: 0.6044 Loss: 1.799\n",
      "Epoch: 14,  Training accuracy: 0.88822 Loss: 0.323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 10948\n",
       "\tepoch: 14\n",
       "\tepoch_length: 782\n",
       "\tmax_epochs: 14\n",
       "\toutput: 0.5704168081283569\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run more epochs\n",
    "EPOCHS = 4\n",
    "\n",
    "# Determine the max number of epochs to run\n",
    "if trainer.state is not None:\n",
    "    current_epoch = trainer.state.epoch\n",
    "    max_epochs = current_epoch + EPOCHS\n",
    "else:\n",
    "    max_epochs = EPOCHS\n",
    "\n",
    "trainer.run(trainloader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284bfec-db3d-4b4c-bd07-440e9c93e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
